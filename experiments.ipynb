{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f68e9c",
   "metadata": {},
   "source": [
    "# Q-Learning Experiments for Irrigation Scheduling\n",
    "\n",
    "Notebook for running and experimenting with tabular Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d7e44",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c66b914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\User\\Documents\\שנה ג\\py_AI\\irrigation_agent\n",
      "Python files available: ['irr_Qtable.py']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add multiple paths for Jupyter compatibility\n",
    "sys.path.insert(0, '.')\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Debug: show what's available\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "print(\"Python files available:\", [f for f in os.listdir('.') if f.endswith('.py')])\n",
    "\n",
    "import numpy as np\n",
    "from irrigation_env import IrrigationEnv\n",
    "from irr_Qtable import train_q_learning, discretize_state, get_state_space_size, N_ACTIONS, from_discrate_to_full_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db4233a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created\n",
      "Action space: Discrete(3)\n",
      "Observation space: Dict('crop_stage': Discrete(3), 'et0': Box(0.0, 1.0, (1,), float32), 'rain': Box(0.0, 1.0, (1,), float32), 'soil_moisture': Box(0.0, 1.0, (1,), float32))\n"
     ]
    }
   ],
   "source": [
    "# Create environment instance\n",
    "env = IrrigationEnv(\n",
    "    max_et0=8.0,\n",
    "    max_rain=50.0,\n",
    "    et0_range=(2.0, 8.0),\n",
    "    rain_range=(0.0, 40.0),\n",
    "    episode_length=90,\n",
    ")\n",
    "\n",
    "print(f\"Environment created\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53931ac4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ba140b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent for 1 episodes...\n",
      "State space size: 60\n",
      "Action space size: 3\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "n_episodes = 1\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.995\n",
    "n_soil_bins = 5\n",
    "\n",
    "print(f\"Training Q-learning agent for {n_episodes} episodes...\")\n",
    "print(f\"State space size: {get_state_space_size(n_soil_bins)}\")\n",
    "print(f\"Action space size: {N_ACTIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33a5f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: State 25, Action 0, Reward 2.0\n",
      "Step 2: State 49, Action 2, Reward -5.8\n",
      "Step 3: State 49, Action 1, Reward -0.8\n",
      "Step 4: State 49, Action 2, Reward -1.8\n",
      "Step 5: State 51, Action 0, Reward -0.30000000000000004\n",
      "Step 6: State 51, Action 1, Reward -0.8\n",
      "Step 7: State 49, Action 0, Reward -0.30000000000000004\n",
      "Step 8: State 50, Action 1, Reward -0.8\n",
      "Step 9: State 51, Action 2, Reward -1.8\n",
      "Step 10: State 51, Action 1, Reward -0.8\n",
      "Step 11: State 51, Action 0, Reward -0.30000000000000004\n",
      "Step 12: State 49, Action 1, Reward -0.8\n",
      "Step 13: State 51, Action 2, Reward -1.8\n",
      "Step 14: State 51, Action 0, Reward -0.30000000000000004\n",
      "Step 15: State 49, Action 0, Reward -0.30000000000000004\n",
      "Step 16: State 49, Action 2, Reward -1.8\n",
      "Step 17: State 51, Action 1, Reward -0.8\n",
      "Step 18: State 50, Action 2, Reward -1.8\n",
      "Step 19: State 49, Action 2, Reward -1.8\n",
      "Step 20: State 51, Action 2, Reward -1.8\n",
      "Step 21: State 49, Action 0, Reward -0.30000000000000004\n",
      "Step 22: State 51, Action 2, Reward -1.8\n",
      "Step 23: State 49, Action 2, Reward -1.8\n",
      "Step 24: State 49, Action 1, Reward -0.8\n",
      "Step 25: State 49, Action 0, Reward -0.30000000000000004\n",
      "Step 26: State 51, Action 0, Reward -0.30000000000000004\n",
      "Step 27: State 51, Action 2, Reward -1.8\n",
      "Step 28: State 51, Action 0, Reward -0.30000000000000004\n",
      "Step 29: State 50, Action 2, Reward -1.8\n",
      "Step 30: State 55, Action 1, Reward -0.8\n",
      "Step 31: State 55, Action 2, Reward -1.8\n",
      "Step 32: State 55, Action 1, Reward -0.8\n",
      "Step 33: State 55, Action 2, Reward -1.8\n",
      "Step 34: State 55, Action 1, Reward -0.8\n",
      "Step 35: State 55, Action 2, Reward -1.8\n",
      "Step 36: State 55, Action 2, Reward -1.8\n",
      "Step 37: State 55, Action 2, Reward -1.8\n",
      "Step 38: State 55, Action 0, Reward -0.30000000000000004\n",
      "Step 39: State 53, Action 0, Reward -0.30000000000000004\n",
      "Step 40: State 55, Action 0, Reward -0.30000000000000004\n",
      "Step 41: State 55, Action 2, Reward -1.8\n",
      "Step 42: State 55, Action 0, Reward -0.30000000000000004\n",
      "Step 43: State 53, Action 2, Reward -1.8\n",
      "Step 44: State 55, Action 0, Reward -0.30000000000000004\n",
      "Step 45: State 55, Action 1, Reward -0.8\n",
      "Step 46: State 55, Action 1, Reward -0.8\n",
      "Step 47: State 53, Action 0, Reward -0.30000000000000004\n",
      "Step 48: State 55, Action 2, Reward -1.8\n",
      "Step 49: State 55, Action 0, Reward -0.30000000000000004\n",
      "Step 50: State 53, Action 2, Reward -1.8\n",
      "Step 51: State 55, Action 0, Reward -0.30000000000000004\n",
      "Step 52: State 55, Action 1, Reward -0.8\n",
      "Step 53: State 53, Action 0, Reward -0.30000000000000004\n",
      "Step 54: State 54, Action 2, Reward -1.8\n",
      "Step 55: State 53, Action 1, Reward -0.8\n",
      "Step 56: State 55, Action 0, Reward -0.30000000000000004\n",
      "Step 57: State 55, Action 0, Reward -0.30000000000000004\n",
      "Step 58: State 53, Action 0, Reward -0.30000000000000004\n",
      "Step 59: State 55, Action 2, Reward -1.8\n",
      "Step 60: State 57, Action 0, Reward -0.30000000000000004\n",
      "Step 61: State 59, Action 2, Reward -1.8\n",
      "Step 62: State 59, Action 2, Reward -1.8\n",
      "Step 63: State 58, Action 1, Reward -0.8\n",
      "Step 64: State 57, Action 0, Reward -0.271434602610944\n",
      "Step 65: State 56, Action 0, Reward -0.30000000000000004\n",
      "Step 66: State 59, Action 1, Reward -0.8\n",
      "Step 67: State 59, Action 2, Reward -1.8\n",
      "Step 68: State 59, Action 0, Reward -0.30000000000000004\n",
      "Step 69: State 59, Action 0, Reward -0.30000000000000004\n",
      "Step 70: State 59, Action 0, Reward -0.30000000000000004\n",
      "Step 71: State 57, Action 0, Reward -0.30000000000000004\n",
      "Step 72: State 59, Action 0, Reward -0.30000000000000004\n",
      "Step 73: State 59, Action 0, Reward -0.30000000000000004\n",
      "Step 74: State 57, Action 0, Reward -0.30000000000000004\n",
      "Step 75: State 59, Action 0, Reward -0.30000000000000004\n",
      "Step 76: State 57, Action 2, Reward -1.8\n",
      "Step 77: State 57, Action 0, Reward -0.30000000000000004\n",
      "Step 78: State 57, Action 1, Reward -0.8\n",
      "Step 79: State 59, Action 1, Reward -0.8\n",
      "Step 80: State 57, Action 1, Reward -0.8\n",
      "Step 81: State 57, Action 1, Reward -0.8\n",
      "Step 82: State 59, Action 1, Reward -0.8\n",
      "Step 83: State 59, Action 2, Reward -1.8\n",
      "Step 84: State 59, Action 2, Reward -1.8\n",
      "Step 85: State 56, Action 0, Reward -0.30000000000000004\n",
      "Step 86: State 59, Action 0, Reward -0.30000000000000004\n",
      "Step 87: State 59, Action 2, Reward -1.8\n",
      "Step 88: State 57, Action 0, Reward -0.30000000000000004\n",
      "Step 89: State 57, Action 0, Reward -0.30000000000000004\n",
      "Step 90: State 59, Action 2, Reward -1.8\n",
      "episode 1/1 , total reward -85.67143460261079, epsilon 0.9950\n",
      "\n",
      "Training complete!\n",
      "Q-table shape: (60, 3)\n",
      "Non-zero entries: 25/180\n",
      "\n",
      "Training complete!\n",
      "Q-table shape: (60, 3)\n",
      "Non-zero entries: 25/180\n"
     ]
    }
   ],
   "source": [
    "# Train Q-learning\n",
    "Q = train_q_learning(\n",
    "    env=env,\n",
    "    n_episodes=n_episodes,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    epsilon_start=epsilon_start,\n",
    "    epsilon_end=epsilon_end,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    n_soil_bins=n_soil_bins,\n",
    "    verbose= True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Q-table shape: {Q.shape}\")\n",
    "print(f\"Non-zero entries: {np.count_nonzero(Q)}/{Q.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb129c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 0: soil=0.18, reward=-0.59\n",
      "Action 1: soil=0.57, reward=5.50\n",
      "Action 2: soil=0.86, reward=-5.66\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.soil_moisture = 0.25\n",
    "env.prev_soil_moisture = 0.25\n",
    "env.current_et0 = 6.0\n",
    "env.current_rain = 0.0\n",
    "env.crop_stage = 1  # flowering\n",
    "\n",
    "obs, _ = env._get_obs(), None\n",
    "\n",
    "for a in [0, 1, 2]:\n",
    "    env.prev_soil_moisture = env.soil_moisture\n",
    "    env._update_state(a)\n",
    "    r = env._calculate_reward(a)\n",
    "    print(f\"Action {a}: soil={env.soil_moisture:.2f}, reward={r:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d50557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _calculate_reward(self, action: int) -> float:\n",
      "        \"\"\"\n",
      "        Calculate transition-based reward signal for the current step.\n",
      "        \n",
      "        1. Transition bonus: +2.0 when ENTERING optimal range (prev not optimal, current is)\n",
      "        2. Stay bonus: +0.5 when STAYING in optimal range\n",
      "        3. Drought penalty: -2.0 per unit below optimal minimum\n",
      "        4. Over-saturation penalty: -0.5 per unit above optimal maximum\n",
      "        5. Irrigation cost penalty: -0.1 per mm irrigated\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        action : int\n",
      "            Irrigation action {0, 1, 2}\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        reward : float\n",
      "            Reward signal for this step (does not modify any state variables)\n",
      "        \"\"\"\n",
      "        \n",
      "        \n",
      "\n",
      "        # Penalty for water stress (soil completely dry)\n",
      "        # old code\n",
      "        \"\"\"\n",
      "        stress_penalty = 0.0\n",
      "        if self.soil_moisture <= self.threshold_bottom_soil_moisture:\n",
      "            stress_penalty = -80 * (self.threshold_bottom_soil_moisture - self.soil_moisture)\n",
      "        if self.soil_moisture >= self.threshold_top_soil_moisture:\n",
      "            stress_penalty += -0.5 * (self.soil_moisture - self.threshold_top_soil_moisture)\n",
      "        if self.threshold_bottom_soil_moisture < self.soil_moisture < self.threshold_top_soil_moisture:\n",
      "            stress_penalty += 4  # Bonus for optimal moisture\n",
      "        \n",
      "        # Penalty for irrigation cost (water usage)\n",
      "        irrigation_amount = self.irrigation_amounts[action]\n",
      "        irrigation_cost_penalty = -self.water_cost * irrigation_amount\n",
      "        #reward -= self.water_cost * irrigation_amount\n",
      "        # Total reward\n",
      "        reward = stress_penalty + irrigation_cost_penalty\n",
      "        \"\"\"\n",
      "        reward = 0 \n",
      "        prev = self.prev_soil_moisture\n",
      "        curr = self.soil_moisture\n",
      "        bottom = self.threshold_bottom_soil_moisture\n",
      "        top = self.threshold_top_soil_moisture\n",
      "\n",
      "        # entering optimum \n",
      "        if prev < bottom and bottom <= curr <= top:\n",
      "            reward += 6\n",
      "\n",
      "        # staying in optimal \n",
      "        if bottom <= prev <= top and bottom <= curr <= top:\n",
      "            reward += 2\n",
      "\n",
      "        # Leaving optimal zone\n",
      "        if bottom <= prev <= top and (curr < bottom or curr > top):\n",
      "            reward -= 4.0\n",
      "\n",
      "        # ---------- 2. Continuous stress penalties ----------\n",
      "        # Below optimal\n",
      "        if curr < bottom:\n",
      "            reward -= 5.0 * (bottom - curr)\n",
      "\n",
      "        # Above optimal\n",
      "        if curr > top:\n",
      "            reward -= 1.0 * (curr - top)\n",
      "\n",
      "        # ---------- 3. Irrigation cost ----------\n",
      "        irrigation_amount = self.irrigation_amounts[action]\n",
      "        reward -= self.water_cost * irrigation_amount\n",
      "    \n",
      "    \n",
      "        return reward\n",
      "\n",
      "bottom/top: 0.3 0.7\n",
      "water_cost: 0.1 irrigation_amounts: [ 0.  5. 15.]\n",
      "One step reward: -4.5973955948611325\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(env._calculate_reward))\n",
    "print(\"bottom/top:\", env.threshold_bottom_soil_moisture, env.threshold_top_soil_moisture)\n",
    "print(\"water_cost:\", env.water_cost, \"irrigation_amounts:\", env.irrigation_amounts)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "env.prev_soil_moisture = env.soil_moisture\n",
    "obs, r, term, trunc, info = env.step(1)\n",
    "print(\"One step reward:\", r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "060c1ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 0, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(from_discrate_to_full_state(state_index=51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d90f6009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3611389992719203 1.0\n",
      "1 0.354290023093221 0.5\n",
      "2 0.6112338395373782 -0.5\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for a in [0, 1, 2]:\n",
    "    env.prev_soil_moisture = 0.28\n",
    "    env.soil_moisture = 0.28\n",
    "    env._update_state(a)\n",
    "    r = env._calculate_reward(a)\n",
    "    print(a, env.soil_moisture, r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50458abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25 30\n",
      "0.29 30\n",
      "0.31 42\n",
      "0.35 42\n",
      "0.65 78\n",
      "0.71 90\n"
     ]
    }
   ],
   "source": [
    "for m in [0.25, 0.29, 0.31, 0.35, 0.65, 0.71]:\n",
    "    obs = {\n",
    "        \"soil_moisture\": np.array([m]),\n",
    "        \"crop_stage\": 1,\n",
    "        \"et0\": np.array([0.5]),\n",
    "        \"rain\": np.array([0.0]),\n",
    "    }\n",
    "    s = discretize_state(obs, n_soil_bins=10)\n",
    "    print(m, s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b7be0",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c623478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table Statistics:\n",
      "Shape: (60, 3)\n",
      "Min Q-value: -9.2630\n",
      "Max Q-value: 0.0000\n",
      "Mean Q-value: -2.1424\n",
      "Std Q-value: 3.1739\n",
      "\n",
      "Non-zero entries: 62/180 (34.4%)\n"
     ]
    }
   ],
   "source": [
    "# Basic Q-table statistics\n",
    "print(\"Q-table Statistics:\")\n",
    "print(f\"Shape: {Q.shape}\")\n",
    "print(f\"Min Q-value: {Q.min():.4f}\")\n",
    "print(f\"Max Q-value: {Q.max():.4f}\")\n",
    "print(f\"Mean Q-value: {Q.mean():.4f}\")\n",
    "print(f\"Std Q-value: {Q.std():.4f}\")\n",
    "print(f\"\\nNon-zero entries: {np.count_nonzero(Q)}/{Q.size} ({100*np.count_nonzero(Q)/Q.size:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "748bdebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action preferences (greedy policy):\n",
      "  Action 0: 59 states (98.3%)\n",
      "  Action 1: 1 states (1.7%)\n",
      "  Action 2: 0 states (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Examine action preferences across all states\n",
    "best_actions = np.argmax(Q, axis=1)\n",
    "action_counts = np.bincount(best_actions, minlength=N_ACTIONS)\n",
    "\n",
    "print(\"Action preferences (greedy policy):\")\n",
    "for action_idx, count in enumerate(action_counts):\n",
    "    print(f\"  Action {action_idx}: {count} states ({100*count/Q.shape[0]:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f52b18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing learned policy:\n",
      "  Step 1: state=27, action=0, reward=-0.022, SM=0.745\n",
      "  Step 2: state=37, action=0, reward=-0.051, SM=0.803\n",
      "  Step 3: state=49, action=0, reward=-0.137, SM=0.974\n",
      "  Step 4: state=51, action=0, reward=-0.150, SM=1.000\n",
      "  Step 5: state=51, action=0, reward=-0.150, SM=1.000\n",
      "\n",
      "Total reward (first 10 steps): -1.252\n"
     ]
    }
   ],
   "source": [
    "# Test learned policy on a single episode\n",
    "obs, info = env.reset(seed=123)\n",
    "total_reward = 0.0\n",
    "done = False\n",
    "step_count = 0\n",
    "\n",
    "print(\"Testing learned policy:\")\n",
    "while not done and step_count < 10:\n",
    "    state = discretize_state(obs, n_soil_bins)\n",
    "    action = np.argmax(Q[state])\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "    \n",
    "    if step_count < 5:\n",
    "        print(f\"  Step {step_count+1}: state={state}, action={action}, reward={reward:.3f}, SM={obs['soil_moisture'][0]:.3f}\")\n",
    "    \n",
    "    step_count += 1\n",
    "\n",
    "print(f\"\\nTotal reward (first {step_count} steps): {total_reward:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff61ccd",
   "metadata": {},
   "source": [
    "## Continued Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16e9051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module reloaded\n"
     ]
    }
   ],
   "source": [
    "# Reload module to pick up changes to train_q_learning\n",
    "import importlib\n",
    "import irr_Qtable\n",
    "importlib.reload(irr_Qtable)\n",
    "from irr_Qtable import train_q_learning, discretize_state, get_state_space_size, N_ACTIONS\n",
    "\n",
    "print(\"Module reloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80b4b80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing training for 500 additional episodes...\n",
      "Initial Q-table stats: min=-9.2630, max=0.0000, mean=-2.1424\n",
      "\n",
      "Training complete!\n",
      "Q-table shape: (60, 3)\n",
      "Non-zero entries: 63/180\n",
      "\n",
      "Continued training complete!\n",
      "Updated Q-table stats: min=-9.3588, max=0.0000, mean=-2.2131\n"
     ]
    }
   ],
   "source": [
    "# Continue training from existing Q-table\n",
    "n_additional_episodes = 500\n",
    "\n",
    "print(f\"Continuing training for {n_additional_episodes} additional episodes...\")\n",
    "print(f\"Initial Q-table stats: min={Q.min():.4f}, max={Q.max():.4f}, mean={Q.mean():.4f}\")\n",
    "\n",
    "Q = train_q_learning(\n",
    "    env=env,\n",
    "    n_episodes=n_additional_episodes,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    epsilon_start=0.1,  # Lower exploration for continued training\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.99,\n",
    "    n_soil_bins=n_soil_bins,\n",
    "    Q_init=Q,\n",
    ")\n",
    "\n",
    "print(\"\\nContinued training complete!\")\n",
    "print(f\"Updated Q-table stats: min={Q.min():.4f}, max={Q.max():.4f}, mean={Q.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb794842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action preferences after continued training:\n",
      "  Action 0: 57 states (95.0%)\n",
      "  Action 1: 3 states (5.0%)\n",
      "  Action 2: 0 states (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Compare action preferences after continued training\n",
    "best_actions_updated = np.argmax(Q, axis=1)\n",
    "action_counts_updated = np.bincount(best_actions_updated, minlength=N_ACTIONS)\n",
    "\n",
    "print(\"Action preferences after continued training:\")\n",
    "for action_idx, count in enumerate(action_counts_updated):\n",
    "    print(f\"  Action {action_idx}: {count} states ({100*count/Q.shape[0]:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64c63f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State_index | Q(no)   Q(light)  Q(heavy) | Best\n",
      "------------------------------------------------\n",
      "          0 | 0,0,0,0|  0.000    0.000    0.000 | 0\n",
      "          1 | 0,0,0,1|  0.000    0.000    0.000 | 0\n",
      "          2 | 0,0,1,0|  0.000    0.000    0.000 | 0\n",
      "          3 | 0,0,1,1|  0.000    0.000    0.000 | 0\n",
      "          4 | 0,1,0,0|  0.000    0.000    0.000 | 0\n",
      "          5 | 0,1,0,1|  0.000    0.000    0.000 | 0\n",
      "          6 | 0,1,1,0|  0.000    0.000    0.000 | 0\n",
      "          7 | 0,1,1,1|  0.000    0.000    0.000 | 0\n",
      "          8 | 0,2,0,0|  0.000    0.000    0.000 | 0\n",
      "          9 | 0,2,0,1|  0.000    0.000    0.000 | 0\n",
      "         10 | 0,2,1,0|  0.000    0.000    0.000 | 0\n",
      "         11 | 0,2,1,1|  0.000    0.000    0.000 | 0\n",
      "         12 | 1,0,0,0| -5.868   -6.042   -5.970 | 0\n",
      "         13 | 1,0,0,1| -6.992   -7.704   -8.273 | 0\n",
      "         14 | 1,0,1,0| -5.817   -5.957   -6.093 | 0\n",
      "         15 | 1,0,1,1| -7.040   -7.764   -8.953 | 0\n",
      "         16 | 1,1,0,0|  0.000    0.000    0.000 | 0\n",
      "         17 | 1,1,0,1|  0.000    0.000    0.000 | 0\n",
      "         18 | 1,1,1,0|  0.000    0.000    0.000 | 0\n",
      "         19 | 1,1,1,1|  0.000    0.000    0.000 | 0\n",
      "         20 | 1,2,0,0|  0.000    0.000    0.000 | 0\n",
      "         21 | 1,2,0,1|  0.000    0.000    0.000 | 0\n",
      "         22 | 1,2,1,0|  0.000    0.000    0.000 | 0\n",
      "         23 | 1,2,1,1|  0.000    0.000    0.000 | 0\n",
      "         24 | 2,0,0,0| -7.731   -7.978   -9.222 | 0\n",
      "         25 | 2,0,0,1| -7.851   -8.174   -9.331 | 0\n",
      "         26 | 2,0,1,0| -7.833   -8.356   -9.295 | 0\n",
      "         27 | 2,0,1,1| -7.623   -8.412   -9.210 | 0\n",
      "         28 | 2,1,0,0| -5.945   -6.498   -7.390 | 0\n",
      "         29 | 2,1,0,1| -5.697   -6.468   -7.559 | 0\n",
      "         30 | 2,1,1,0| -5.853   -6.382   -7.532 | 0\n",
      "         31 | 2,1,1,1| -5.917   -6.448   -7.534 | 0\n",
      "         32 | 2,2,0,0| -3.541   -3.748   -4.747 | 0\n",
      "         33 | 2,2,0,1| -3.229   -4.002   -4.978 | 0\n",
      "         34 | 2,2,1,0| -3.340   -4.084   -5.010 | 0\n",
      "         35 | 2,2,1,1| -3.470   -3.988   -4.958 | 0\n"
     ]
    }
   ],
   "source": [
    "print(\"State_index | Q(no)   Q(light)  Q(heavy) | Best\")\n",
    "print(\"------------------------------------------------\")\n",
    "for state in range(Q.shape[0]):\n",
    "    best_action = np.argmax(Q[state])\n",
    "    soil, stage,et0,rain  = from_discrate_to_full_state(state, n_soil_bins)\n",
    "    print(\n",
    "            f\"{state:11d} | \"\n",
    "            f\"{soil},{stage},{et0},{rain}|\"\n",
    "            f\"{Q[state, 0]:7.3f}  {Q[state, 1]:7.3f}  {Q[state, 2]:7.3f} | \"\n",
    "            f\"{best_action}\"\n",
    "        )\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e518b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visited states: [12, 13, 14, 15, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n",
      "Number of visited states: 16\n"
     ]
    }
   ],
   "source": [
    "visited_states = set()\n",
    "\n",
    "for episode in range(200):\n",
    "    obs, _ = env.reset()\n",
    "    state = discretize_state(obs, n_soil_bins=3)\n",
    "    visited_states.add(state)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.random.randint(3)  # exploration מוחלט\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        state = discretize_state(obs, n_soil_bins=3)\n",
    "        visited_states.add(state)\n",
    "\n",
    "print(\"Visited states:\", sorted(visited_states))\n",
    "print(\"Number of visited states:\", len(visited_states))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5471a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State (0, 0, 0, 0) - Action 0 -> Reward -0.60 -> Next State {'soil_moisture': array([0.], dtype=float32), 'crop_stage': 0, 'rain': array([0.0983193], dtype=float32), 'et0': array([0.8345703], dtype=float32)}\n",
      "State (0, 0, 0, 0) - Action 1 -> Reward -0.97 -> Next State {'soil_moisture': array([0.06577684], dtype=float32), 'crop_stage': 0, 'rain': array([0.11071205], dtype=float32), 'et0': array([0.35413218], dtype=float32)}\n",
      "State (0, 0, 0, 0) - Action 2 -> Reward -1.59 -> Next State {'soil_moisture': array([0.25696757], dtype=float32), 'crop_stage': 0, 'rain': array([0.6736061], dtype=float32), 'et0': array([0.77843297], dtype=float32)}\n",
      "State (0, 0, 0, 1) - Action 0 -> Reward 1.00 -> Next State {'soil_moisture': array([0.5], dtype=float32), 'crop_stage': 0, 'rain': array([0.12238463], dtype=float32), 'et0': array([0.6364257], dtype=float32)}\n",
      "State (0, 0, 0, 1) - Action 1 -> Reward 0.50 -> Next State {'soil_moisture': array([0.58573526], dtype=float32), 'crop_stage': 0, 'rain': array([0.07755982], dtype=float32), 'et0': array([0.8910592], dtype=float32)}\n",
      "State (0, 0, 0, 1) - Action 2 -> Reward -1.52 -> Next State {'soil_moisture': array([0.7388728], dtype=float32), 'crop_stage': 0, 'rain': array([0.5927328], dtype=float32), 'et0': array([0.35487777], dtype=float32)}\n",
      "State (0, 0, 1, 0) - Action 0 -> Reward -0.60 -> Next State {'soil_moisture': array([0.], dtype=float32), 'crop_stage': 0, 'rain': array([0.74498665], dtype=float32), 'et0': array([0.9860219], dtype=float32)}\n",
      "State (0, 0, 1, 0) - Action 1 -> Reward 0.50 -> Next State {'soil_moisture': array([0.38305244], dtype=float32), 'crop_stage': 0, 'rain': array([0.40273103], dtype=float32), 'et0': array([0.7014208], dtype=float32)}\n",
      "State (0, 0, 1, 0) - Action 2 -> Reward -1.50 -> Next State {'soil_moisture': array([0.7063611], dtype=float32), 'crop_stage': 0, 'rain': array([0.6917387], dtype=float32), 'et0': array([0.8315888], dtype=float32)}\n",
      "State (0, 0, 1, 1) - Action 0 -> Reward 1.00 -> Next State {'soil_moisture': array([0.46], dtype=float32), 'crop_stage': 0, 'rain': array([0.68416274], dtype=float32), 'et0': array([0.5863885], dtype=float32)}\n",
      "State (0, 0, 1, 1) - Action 1 -> Reward -0.56 -> Next State {'soil_moisture': array([0.8286258], dtype=float32), 'crop_stage': 0, 'rain': array([0.2421069], dtype=float32), 'et0': array([0.38102365], dtype=float32)}\n",
      "State (0, 0, 1, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.5951774], dtype=float32), 'et0': array([0.4325475], dtype=float32)}\n",
      "State (0, 1, 0, 0) - Action 0 -> Reward -0.60 -> Next State {'soil_moisture': array([0.], dtype=float32), 'crop_stage': 0, 'rain': array([0.41287962], dtype=float32), 'et0': array([0.65734816], dtype=float32)}\n",
      "State (0, 1, 0, 0) - Action 1 -> Reward -0.64 -> Next State {'soil_moisture': array([0.23014587], dtype=float32), 'crop_stage': 0, 'rain': array([0.04846581], dtype=float32), 'et0': array([0.597727], dtype=float32)}\n",
      "State (0, 1, 0, 0) - Action 2 -> Reward -0.50 -> Next State {'soil_moisture': array([0.3804697], dtype=float32), 'crop_stage': 0, 'rain': array([0.11764699], dtype=float32), 'et0': array([0.81381047], dtype=float32)}\n",
      "State (0, 1, 0, 1) - Action 0 -> Reward 1.00 -> Next State {'soil_moisture': array([0.5], dtype=float32), 'crop_stage': 0, 'rain': array([0.7845267], dtype=float32), 'et0': array([0.7108932], dtype=float32)}\n",
      "State (0, 1, 0, 1) - Action 1 -> Reward -0.61 -> Next State {'soil_moisture': array([0.9138276], dtype=float32), 'crop_stage': 0, 'rain': array([0.59568727], dtype=float32), 'et0': array([0.5410604], dtype=float32)}\n",
      "State (0, 1, 0, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.34378418], dtype=float32), 'et0': array([0.63431674], dtype=float32)}\n",
      "State (0, 1, 1, 0) - Action 0 -> Reward -0.60 -> Next State {'soil_moisture': array([0.], dtype=float32), 'crop_stage': 0, 'rain': array([0.08338352], dtype=float32), 'et0': array([0.34315547], dtype=float32)}\n",
      "State (0, 1, 1, 0) - Action 1 -> Reward -0.94 -> Next State {'soil_moisture': array([0.07796554], dtype=float32), 'crop_stage': 0, 'rain': array([0.11225208], dtype=float32), 'et0': array([0.64098585], dtype=float32)}\n",
      "State (0, 1, 1, 0) - Action 2 -> Reward -1.58 -> Next State {'soil_moisture': array([0.25845215], dtype=float32), 'crop_stage': 0, 'rain': array([0.28800637], dtype=float32), 'et0': array([0.62865645], dtype=float32)}\n",
      "State (0, 1, 1, 1) - Action 0 -> Reward 1.00 -> Next State {'soil_moisture': array([0.408], dtype=float32), 'crop_stage': 0, 'rain': array([0.75961137], dtype=float32), 'et0': array([0.40768585], dtype=float32)}\n",
      "State (0, 1, 1, 1) - Action 1 -> Reward -0.56 -> Next State {'soil_moisture': array([0.8214983], dtype=float32), 'crop_stage': 0, 'rain': array([0.24186713], dtype=float32), 'et0': array([0.8256113], dtype=float32)}\n",
      "State (0, 1, 1, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.49144265], dtype=float32), 'et0': array([0.37689832], dtype=float32)}\n",
      "State (0, 2, 0, 0) - Action 0 -> Reward -0.60 -> Next State {'soil_moisture': array([0.], dtype=float32), 'crop_stage': 0, 'rain': array([0.6290037], dtype=float32), 'et0': array([0.4009852], dtype=float32)}\n",
      "State (0, 2, 0, 0) - Action 1 -> Reward 0.50 -> Next State {'soil_moisture': array([0.34846243], dtype=float32), 'crop_stage': 0, 'rain': array([0.6969387], dtype=float32), 'et0': array([0.8244879], dtype=float32)}\n",
      "State (0, 2, 0, 0) - Action 2 -> Reward -1.56 -> Next State {'soil_moisture': array([0.81395227], dtype=float32), 'crop_stage': 0, 'rain': array([0.5877857], dtype=float32), 'et0': array([0.37642694], dtype=float32)}\n",
      "State (0, 2, 0, 1) - Action 0 -> Reward 1.00 -> Next State {'soil_moisture': array([0.5], dtype=float32), 'crop_stage': 0, 'rain': array([0.5911374], dtype=float32), 'et0': array([0.3207471], dtype=float32)}\n",
      "State (0, 2, 0, 1) - Action 1 -> Reward -0.57 -> Next State {'soil_moisture': array([0.8327388], dtype=float32), 'crop_stage': 0, 'rain': array([0.31304276], dtype=float32), 'et0': array([0.67309624], dtype=float32)}\n",
      "State (0, 2, 0, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.60303086], dtype=float32), 'et0': array([0.9731996], dtype=float32)}\n",
      "State (0, 2, 1, 0) - Action 0 -> Reward -0.60 -> Next State {'soil_moisture': array([0.], dtype=float32), 'crop_stage': 0, 'rain': array([0.26528493], dtype=float32), 'et0': array([0.52140963], dtype=float32)}\n",
      "State (0, 2, 1, 0) - Action 1 -> Reward -0.78 -> Next State {'soil_moisture': array([0.16178608], dtype=float32), 'crop_stage': 0, 'rain': array([0.49507245], dtype=float32), 'et0': array([0.27230066], dtype=float32)}\n",
      "State (0, 2, 1, 0) - Action 2 -> Reward -0.50 -> Next State {'soil_moisture': array([0.54843026], dtype=float32), 'crop_stage': 0, 'rain': array([0.3328904], dtype=float32), 'et0': array([0.3768297], dtype=float32)}\n",
      "State (0, 2, 1, 1) - Action 0 -> Reward 1.00 -> Next State {'soil_moisture': array([0.444], dtype=float32), 'crop_stage': 0, 'rain': array([0.69004124], dtype=float32), 'et0': array([0.3830641], dtype=float32)}\n",
      "State (0, 2, 1, 1) - Action 1 -> Reward -0.56 -> Next State {'soil_moisture': array([0.82369804], dtype=float32), 'crop_stage': 0, 'rain': array([0.47838226], dtype=float32), 'et0': array([0.3642088], dtype=float32)}\n",
      "State (0, 2, 1, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.05176414], dtype=float32), 'et0': array([0.33534795], dtype=float32)}\n",
      "State (1, 0, 0, 0) - Action 0 -> Reward 1.00 -> Next State {'soil_moisture': array([0.5], dtype=float32), 'crop_stage': 0, 'rain': array([0.22382122], dtype=float32), 'et0': array([0.7414759], dtype=float32)}\n",
      "State (1, 0, 0, 0) - Action 1 -> Reward 0.50 -> Next State {'soil_moisture': array([0.63225156], dtype=float32), 'crop_stage': 0, 'rain': array([0.11938233], dtype=float32), 'et0': array([0.9401921], dtype=float32)}\n",
      "State (1, 0, 0, 0) - Action 2 -> Reward -1.55 -> Next State {'soil_moisture': array([0.80433506], dtype=float32), 'crop_stage': 0, 'rain': array([0.68188435], dtype=float32), 'et0': array([0.3901441], dtype=float32)}\n",
      "State (1, 0, 0, 1) - Action 0 -> Reward -0.15 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.18207367], dtype=float32), 'et0': array([0.64195937], dtype=float32)}\n",
      "State (1, 0, 0, 1) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.21982892], dtype=float32), 'et0': array([0.91567755], dtype=float32)}\n",
      "State (1, 0, 0, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.7330229], dtype=float32), 'et0': array([0.41738638], dtype=float32)}\n",
      "State (1, 0, 1, 0) - Action 0 -> Reward 1.00 -> Next State {'soil_moisture': array([0.46], dtype=float32), 'crop_stage': 0, 'rain': array([0.794305], dtype=float32), 'et0': array([0.792803], dtype=float32)}\n",
      "State (1, 0, 1, 0) - Action 1 -> Reward -0.59 -> Next State {'soil_moisture': array([0.8754404], dtype=float32), 'crop_stage': 0, 'rain': array([0.67577195], dtype=float32), 'et0': array([0.88336134], dtype=float32)}\n",
      "State (1, 0, 1, 0) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.23958023], dtype=float32), 'et0': array([0.4326405], dtype=float32)}\n",
      "State (1, 0, 1, 1) - Action 0 -> Reward -0.13 -> Next State {'soil_moisture': array([0.96], dtype=float32), 'crop_stage': 0, 'rain': array([0.56113696], dtype=float32), 'et0': array([0.7032871], dtype=float32)}\n",
      "State (1, 0, 1, 1) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.79025406], dtype=float32), 'et0': array([0.5020048], dtype=float32)}\n",
      "State (1, 0, 1, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.59686327], dtype=float32), 'et0': array([0.48829517], dtype=float32)}\n",
      "State (1, 1, 0, 0) - Action 0 -> Reward 1.00 -> Next State {'soil_moisture': array([0.5], dtype=float32), 'crop_stage': 0, 'rain': array([0.7737675], dtype=float32), 'et0': array([0.8914865], dtype=float32)}\n",
      "State (1, 1, 0, 0) - Action 1 -> Reward -0.60 -> Next State {'soil_moisture': array([0.90122426], dtype=float32), 'crop_stage': 0, 'rain': array([0.78806216], dtype=float32), 'et0': array([0.8645868], dtype=float32)}\n",
      "State (1, 1, 0, 0) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.50114274], dtype=float32), 'et0': array([0.6499718], dtype=float32)}\n",
      "State (1, 1, 0, 1) - Action 0 -> Reward -0.15 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.5473267], dtype=float32), 'et0': array([0.6291333], dtype=float32)}\n",
      "State (1, 1, 0, 1) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.4768339], dtype=float32), 'et0': array([0.36258876], dtype=float32)}\n",
      "State (1, 1, 0, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.44697675], dtype=float32), 'et0': array([0.78340006], dtype=float32)}\n",
      "State (1, 1, 1, 0) - Action 0 -> Reward 1.00 -> Next State {'soil_moisture': array([0.408], dtype=float32), 'crop_stage': 0, 'rain': array([0.49441117], dtype=float32), 'et0': array([0.86056274], dtype=float32)}\n",
      "State (1, 1, 1, 0) - Action 1 -> Reward 0.50 -> Next State {'soil_moisture': array([0.67078304], dtype=float32), 'crop_stage': 0, 'rain': array([0.17143689], dtype=float32), 'et0': array([0.590869], dtype=float32)}\n",
      "State (1, 1, 1, 0) - Action 2 -> Reward -1.59 -> Next State {'soil_moisture': array([0.88286674], dtype=float32), 'crop_stage': 0, 'rain': array([0.40331435], dtype=float32), 'et0': array([0.59114206], dtype=float32)}\n",
      "State (1, 1, 1, 1) - Action 0 -> Reward -0.10 -> Next State {'soil_moisture': array([0.908], dtype=float32), 'crop_stage': 0, 'rain': array([0.38023013], dtype=float32), 'et0': array([0.7961902], dtype=float32)}\n",
      "State (1, 1, 1, 1) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.7837204], dtype=float32), 'et0': array([0.72571445], dtype=float32)}\n",
      "State (1, 1, 1, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.27917904], dtype=float32), 'et0': array([0.4140392], dtype=float32)}\n",
      "State (1, 2, 0, 0) - Action 0 -> Reward 1.00 -> Next State {'soil_moisture': array([0.5], dtype=float32), 'crop_stage': 0, 'rain': array([0.46900275], dtype=float32), 'et0': array([0.93136656], dtype=float32)}\n",
      "State (1, 2, 0, 0) - Action 1 -> Reward -0.52 -> Next State {'soil_moisture': array([0.74724674], dtype=float32), 'crop_stage': 0, 'rain': array([0.579547], dtype=float32), 'et0': array([0.7083155], dtype=float32)}\n",
      "State (1, 2, 0, 0) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.08953633], dtype=float32), 'et0': array([0.71806455], dtype=float32)}\n",
      "State (1, 2, 0, 1) - Action 0 -> Reward -0.15 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.18278608], dtype=float32), 'et0': array([0.89073104], dtype=float32)}\n",
      "State (1, 2, 0, 1) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.29491872], dtype=float32), 'et0': array([0.69966966], dtype=float32)}\n",
      "State (1, 2, 0, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.7631294], dtype=float32), 'et0': array([0.46353072], dtype=float32)}\n",
      "State (1, 2, 1, 0) - Action 0 -> Reward 1.00 -> Next State {'soil_moisture': array([0.444], dtype=float32), 'crop_stage': 0, 'rain': array([0.28669932], dtype=float32), 'et0': array([0.83046776], dtype=float32)}\n",
      "State (1, 2, 1, 0) - Action 1 -> Reward 0.50 -> Next State {'soil_moisture': array([0.6041309], dtype=float32), 'crop_stage': 0, 'rain': array([0.24304835], dtype=float32), 'et0': array([0.87859124], dtype=float32)}\n",
      "State (1, 2, 1, 0) - Action 2 -> Reward -1.57 -> Next State {'soil_moisture': array([0.8405115], dtype=float32), 'crop_stage': 0, 'rain': array([0.747802], dtype=float32), 'et0': array([0.5688533], dtype=float32)}\n",
      "State (1, 2, 1, 1) - Action 0 -> Reward -0.12 -> Next State {'soil_moisture': array([0.944], dtype=float32), 'crop_stage': 0, 'rain': array([0.09137166], dtype=float32), 'et0': array([0.69065636], dtype=float32)}\n",
      "State (1, 2, 1, 1) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.7562441], dtype=float32), 'et0': array([0.43180272], dtype=float32)}\n",
      "State (1, 2, 1, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.0044897], dtype=float32), 'et0': array([0.58287364], dtype=float32)}\n",
      "State (2, 0, 0, 0) - Action 0 -> Reward -0.15 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.31307393], dtype=float32), 'et0': array([0.48375604], dtype=float32)}\n",
      "State (2, 0, 0, 0) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.05089084], dtype=float32), 'et0': array([0.27792877], dtype=float32)}\n",
      "State (2, 0, 0, 0) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.35768527], dtype=float32), 'et0': array([0.34725282], dtype=float32)}\n",
      "State (2, 0, 0, 1) - Action 0 -> Reward -0.15 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.7791308], dtype=float32), 'et0': array([0.8734852], dtype=float32)}\n",
      "State (2, 0, 0, 1) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.53143114], dtype=float32), 'et0': array([0.49618298], dtype=float32)}\n",
      "State (2, 0, 0, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.43920112], dtype=float32), 'et0': array([0.76813716], dtype=float32)}\n",
      "State (2, 0, 1, 0) - Action 0 -> Reward -0.13 -> Next State {'soil_moisture': array([0.96], dtype=float32), 'crop_stage': 0, 'rain': array([0.3845685], dtype=float32), 'et0': array([0.3619293], dtype=float32)}\n",
      "State (2, 0, 1, 0) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.68073434], dtype=float32), 'et0': array([0.47311908], dtype=float32)}\n",
      "State (2, 0, 1, 0) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.1794765], dtype=float32), 'et0': array([0.7025313], dtype=float32)}\n",
      "State (2, 0, 1, 1) - Action 0 -> Reward -0.15 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.17990279], dtype=float32), 'et0': array([0.37240517], dtype=float32)}\n",
      "State (2, 0, 1, 1) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.65889245], dtype=float32), 'et0': array([0.33504358], dtype=float32)}\n",
      "State (2, 0, 1, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.45435753], dtype=float32), 'et0': array([0.7023418], dtype=float32)}\n",
      "State (2, 1, 0, 0) - Action 0 -> Reward -0.15 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.20029502], dtype=float32), 'et0': array([0.54377365], dtype=float32)}\n",
      "State (2, 1, 0, 0) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.61340046], dtype=float32), 'et0': array([0.7733168], dtype=float32)}\n",
      "State (2, 1, 0, 0) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.6428997], dtype=float32), 'et0': array([0.45889112], dtype=float32)}\n",
      "State (2, 1, 0, 1) - Action 0 -> Reward -0.15 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.6921789], dtype=float32), 'et0': array([0.732625], dtype=float32)}\n",
      "State (2, 1, 0, 1) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.1546424], dtype=float32), 'et0': array([0.38702288], dtype=float32)}\n",
      "State (2, 1, 0, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.34221798], dtype=float32), 'et0': array([0.323153], dtype=float32)}\n",
      "State (2, 1, 1, 0) - Action 0 -> Reward -0.10 -> Next State {'soil_moisture': array([0.908], dtype=float32), 'crop_stage': 0, 'rain': array([0.7945028], dtype=float32), 'et0': array([0.89751], dtype=float32)}\n",
      "State (2, 1, 1, 0) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.5440759], dtype=float32), 'et0': array([0.6118532], dtype=float32)}\n",
      "State (2, 1, 1, 0) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.7080708], dtype=float32), 'et0': array([0.7993705], dtype=float32)}\n",
      "State (2, 1, 1, 1) - Action 0 -> Reward -0.15 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.5857707], dtype=float32), 'et0': array([0.85447145], dtype=float32)}\n",
      "State (2, 1, 1, 1) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.43005118], dtype=float32), 'et0': array([0.78857535], dtype=float32)}\n",
      "State (2, 1, 1, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.5469163], dtype=float32), 'et0': array([0.6341726], dtype=float32)}\n",
      "State (2, 2, 0, 0) - Action 0 -> Reward -0.15 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.17020187], dtype=float32), 'et0': array([0.39902192], dtype=float32)}\n",
      "State (2, 2, 0, 0) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.32378343], dtype=float32), 'et0': array([0.7146322], dtype=float32)}\n",
      "State (2, 2, 0, 0) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.39305288], dtype=float32), 'et0': array([0.6948821], dtype=float32)}\n",
      "State (2, 2, 0, 1) - Action 0 -> Reward -0.15 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.05991258], dtype=float32), 'et0': array([0.2605048], dtype=float32)}\n",
      "State (2, 2, 0, 1) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.15167539], dtype=float32), 'et0': array([0.9624367], dtype=float32)}\n",
      "State (2, 2, 0, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.3081147], dtype=float32), 'et0': array([0.8166043], dtype=float32)}\n",
      "State (2, 2, 1, 0) - Action 0 -> Reward -0.12 -> Next State {'soil_moisture': array([0.944], dtype=float32), 'crop_stage': 0, 'rain': array([0.6384064], dtype=float32), 'et0': array([0.76091754], dtype=float32)}\n",
      "State (2, 2, 1, 0) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.29412577], dtype=float32), 'et0': array([0.54980844], dtype=float32)}\n",
      "State (2, 2, 1, 0) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.7072055], dtype=float32), 'et0': array([0.41355148], dtype=float32)}\n",
      "State (2, 2, 1, 1) - Action 0 -> Reward -0.15 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.3201536], dtype=float32), 'et0': array([0.63965875], dtype=float32)}\n",
      "State (2, 2, 1, 1) - Action 1 -> Reward -0.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.09880079], dtype=float32), 'et0': array([0.76265347], dtype=float32)}\n",
      "State (2, 2, 1, 1) - Action 2 -> Reward -1.65 -> Next State {'soil_moisture': array([1.], dtype=float32), 'crop_stage': 0, 'rain': array([0.10333545], dtype=float32), 'et0': array([0.4485461], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# הגדרת הפעולות\n",
    "actions = [0, 1, 2]  # 0 = no irrigation, 1 = light, 2 = heavy irrigation\n",
    "\n",
    "# ביצוע מעבר מצב עבור כל פעולה, לכל מצב\n",
    "for soil_bin in range(3):  # 0, 1, 2 for soil moisture\n",
    "    for crop_stage in range(3):  # 0, 1, 2 for crop stages (emergence, flowering, maturity)\n",
    "        for et0_bin in range(2):  # 0, 1 for ET0 values (low/high)\n",
    "            for rain_bin in range(2):  # 0, 1 for rain values (no/yes)\n",
    "                \n",
    "                # הגדרת המצב הנוכחי\n",
    "                state = (soil_bin, crop_stage, et0_bin, rain_bin)\n",
    "                \n",
    "                # אתחול ה-env לפי המצב הנוכחי\n",
    "                env.soil_moisture = state[0] * 0.5  # לדוגמה, אם soil_bin = 1 אז רטיבות 0.5\n",
    "                env.crop_stage = state[1]\n",
    "                env.current_step = 0  # אתחול הזמן למצב התחלה\n",
    "                env.current_et0 = state[2] * 8.0  # לדוגמה, אם et0_bin = 1 אז et0 = 8\n",
    "                env.current_rain = state[3] * 50.0  # אם rain_bin = 1 אז גשם = 50\n",
    "\n",
    "                for action in actions:\n",
    "                    # הפעלת פעולה על המצב הנוכחי\n",
    "                    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "                    # הדפסת המעבר (המצב הנוכחי, פעולה, תגמול, מצב הבא)\n",
    "                    print(f\"State {state} - Action {action} -> Reward {reward:.2f} -> Next State {next_state}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
