{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5231f26",
   "metadata": {},
   "source": [
    "# Q-Learning Irrigation Experiments - Historical Archive\n",
    "\n",
    "This notebook contains archived experiments from the Q-learning irrigation project, including:\n",
    "- Physical stability calibration for soil moisture bins\n",
    "- Monitored training loops with Q-value evolution tracking\n",
    "- Rain regime comparison experiments (dry vs moderate)\n",
    "- State coverage experiments and debugging\n",
    "\n",
    "**Purpose:** Historical record of experimental exploration leading to final methodology.\n",
    "\n",
    "**Note:** For the final, clean experimental workflow, see `experiments.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02633731",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a88ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment instance with stability-calibrated parameters\n",
    "# Parameters chosen via physical stability experiments (see Stability Calibration section below)\n",
    "# to ensure soil_bin=1 is dynamically stable (mean residence ≥10 steps)\n",
    "env = IrrigationEnv(\n",
    "    max_et0=8.0,\n",
    "    max_rain=50.0,\n",
    "    et0_range=(2.0, 8.0),\n",
    "    rain_range=(0.0, 0.8),           # Calibrated (was 40.0) - reduces perturbations\n",
    "    max_soil_moisture=320.0,         # Calibrated (was 100.0) - increases capacity\n",
    "    episode_length=90,\n",
    ")\n",
    "\n",
    "print(f\"Environment created (stability-calibrated)\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5942a89",
   "metadata": {},
   "source": [
    "---\n",
    "# PHYSICAL STABILITY CALIBRATION (Soil Bin 1)\n",
    "\n",
    "**Problem:** Under baseline parameters (rain 0-40mm, capacity 100mm), soil_bin=1 (SM ∈ [0.333, 0.667)) was highly unstable with mean residence time of only **1.62 steps**. This prevented meaningful Q-learning in the mid-moisture range.\n",
    "\n",
    "**Objective:** Make soil_bin=1 dynamically stable under random policy exploration with **mean residence time ≥10 consecutive steps**.\n",
    "\n",
    "**Constraints:**\n",
    "- No drainage, runoff, or percolation modifications\n",
    "- No reward function changes\n",
    "- No discretization changes\n",
    "- Only climate sampling and soil capacity adjustments allowed\n",
    "\n",
    "**Method:** Systematic parameter sweeps testing rain reduction and soil capacity increases.\n",
    "\n",
    "**Results:**\n",
    "\n",
    "| Configuration | rain_range | max_soil_moisture | Mean Residence | Status |\n",
    "|--------------|-----------|-------------------|----------------|--------|\n",
    "| Baseline | (0, 40) | 100 | 1.62 steps | ✗ Unstable |\n",
    "| E3+ Final | (0, 0.8) | 320 | 11.28 steps | ✓ Stable |\n",
    "\n",
    "**Mechanism:** Rain reduction (6× improvement) combined with capacity increase (3× improvement) creates synergistic 10× improvement. Physical stability emerges from ratio: bin_width / max_input_perturbation.\n",
    "\n",
    "**Validation:** 100% success rate across 5 independent trials (range: 10.96-11.59 steps).\n",
    "\n",
    "✅ **Bin 1 is now dynamically stable and suitable for policy learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7742e57e",
   "metadata": {},
   "source": [
    "## Validate Stability in This Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31908478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_bin1_stability(env, n_episodes=100, n_soil_bins=3):\n",
    "    \"\"\"\n",
    "    Measure stability of soil_bin=1 under random policy.\n",
    "    \n",
    "    Returns mean residence time, median, max, and bin occupancy percentage.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    residence_times = []\n",
    "    bin_visits = defaultdict(int)\n",
    "    entry_count = 0\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        in_bin1 = False\n",
    "        current_residence = 0\n",
    "        \n",
    "        while not done:\n",
    "            state_idx = discretize_state(obs, n_soil_bins)\n",
    "            # Extract soil_bin from state index\n",
    "            soil_bin = state_idx // (3 * 2 * 2)  # state = soil_bin * 12 + crop * 4 + et0 * 2 + rain\n",
    "            bin_visits[soil_bin] += 1\n",
    "            \n",
    "            if soil_bin == 1:\n",
    "                if not in_bin1:\n",
    "                    in_bin1 = True\n",
    "                    entry_count += 1\n",
    "                    current_residence = 1\n",
    "                else:\n",
    "                    current_residence += 1\n",
    "            else:\n",
    "                if in_bin1:\n",
    "                    residence_times.append(current_residence)\n",
    "                    in_bin1 = False\n",
    "                    current_residence = 0\n",
    "            \n",
    "            action = np.random.randint(N_ACTIONS)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        if in_bin1:\n",
    "            residence_times.append(current_residence)\n",
    "    \n",
    "    if len(residence_times) > 0:\n",
    "        mean_residence = np.mean(residence_times)\n",
    "        median_residence = np.median(residence_times)\n",
    "        max_residence = np.max(residence_times)\n",
    "    else:\n",
    "        mean_residence = 0\n",
    "        median_residence = 0\n",
    "        max_residence = 0\n",
    "    \n",
    "    total_steps = sum(bin_visits.values())\n",
    "    bin1_percentage = 100 * bin_visits[1] / total_steps if total_steps > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'mean_residence': mean_residence,\n",
    "        'median_residence': median_residence,\n",
    "        'max_residence': max_residence,\n",
    "        'bin1_percentage': bin1_percentage,\n",
    "        'entry_count': entry_count,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72783e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run stability validation with current calibrated parameters\n",
    "print(\"Validating soil_bin=1 stability...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "stats = measure_bin1_stability(env, n_episodes=100, n_soil_bins=3)\n",
    "\n",
    "print(f\"\\n✅ STABILITY VALIDATION RESULTS:\")\n",
    "print(f\"   Mean residence time:    {stats['mean_residence']:.2f} steps\")\n",
    "print(f\"   Median residence time:  {stats['median_residence']:.2f} steps\")\n",
    "print(f\"   Maximum residence time: {stats['max_residence']} steps\")\n",
    "print(f\"   Bin 1 occupancy:        {stats['bin1_percentage']:.1f}%\")\n",
    "print(f\"   Number of entries:      {stats['entry_count']}\")\n",
    "\n",
    "if stats['mean_residence'] >= 10.0:\n",
    "    print(f\"\\n✓ SUCCESS: Bin 1 is dynamically stable (mean ≥ 10 steps)\")\n",
    "else:\n",
    "    print(f\"\\n✗ WARNING: Bin 1 stability below target (mean < 10 steps)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ddcca",
   "metadata": {},
   "source": [
    "---\n",
    "# Q-LEARNING TRAINING WITH MONITORING\n",
    "\n",
    "Train a tabular Q-learning agent on the stability-calibrated environment with comprehensive monitoring of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77790fb2",
   "metadata": {},
   "source": [
    "## Step 1: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11275616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "TRAIN_CONFIG = {\n",
    "    'n_episodes': 4000,\n",
    "    'alpha': 0.1,           # Learning rate\n",
    "    'gamma': 0.95,          # Discount factor\n",
    "    'epsilon_start': 1.0,   # Initial exploration rate\n",
    "    'epsilon_end': 0.01,    # Final exploration rate\n",
    "    'epsilon_decay': 0.995, # Multiplicative decay per episode\n",
    "    'n_soil_bins': 3,       # Discretization (must match env)\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Q-LEARNING TRAINING CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "for key, value in TRAIN_CONFIG.items():\n",
    "    print(f\"  {key:20s} = {value}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nState space size: {get_state_space_size(TRAIN_CONFIG['n_soil_bins'])}\")\n",
    "print(f\"Action space size: {N_ACTIONS}\")\n",
    "print(f\"Q-table shape: ({get_state_space_size(TRAIN_CONFIG['n_soil_bins'])}, {N_ACTIONS})\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d2ba8",
   "metadata": {},
   "source": [
    "## Step 2: Instrumented Training Loop\n",
    "\n",
    "Enhanced training with monitoring hooks for action usage, state visitation, rewards, and Q-value evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def train_q_learning_monitored(env, n_episodes, alpha, gamma, epsilon_start, epsilon_end, \n",
    "                                epsilon_decay, n_soil_bins, log_interval=500):\n",
    "    \"\"\"\n",
    "    Train Q-learning agent with comprehensive monitoring.\n",
    "    \n",
    "    Returns:\n",
    "        Q_table: trained Q-values (state_space_size, n_actions)\n",
    "        monitoring: dict with training metrics\n",
    "    \"\"\"\n",
    "    state_space_size = get_state_space_size(n_soil_bins)\n",
    "    Q = np.zeros((state_space_size, N_ACTIONS))\n",
    "    \n",
    "    # Monitoring structures\n",
    "    monitoring = {\n",
    "        'episode_rewards': [],\n",
    "        'action_counts_per_interval': [],\n",
    "        'state_visit_counts': np.zeros(state_space_size, dtype=int),\n",
    "        'q_stats': {'min': [], 'max': [], 'mean': [], 'nonzero_count': []},\n",
    "        'epsilon_history': [],\n",
    "    }\n",
    "    \n",
    "    epsilon = epsilon_start\n",
    "    action_counts = np.zeros(N_ACTIONS, dtype=int)\n",
    "    \n",
    "    print(f\"\\nStarting training for {n_episodes} episodes...\")\n",
    "    print(f\"Logging interval: every {log_interval} episodes\\n\")\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = discretize_state(obs, n_soil_bins)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Track state visitation\n",
    "            monitoring['state_visit_counts'][state] += 1\n",
    "            \n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(N_ACTIONS)\n",
    "            else:\n",
    "                action = np.argmax(Q[state, :])\n",
    "            \n",
    "            # Track action usage\n",
    "            action_counts[action] += 1\n",
    "            \n",
    "            # Environment step\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = discretize_state(next_obs, n_soil_bins)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Q-learning update\n",
    "            best_next_action = np.argmax(Q[next_state, :])\n",
    "            td_target = reward + gamma * Q[next_state, best_next_action]\n",
    "            td_error = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_error\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        # Record episode metrics\n",
    "        monitoring['episode_rewards'].append(episode_reward)\n",
    "        monitoring['epsilon_history'].append(epsilon)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        \n",
    "        # Periodic logging\n",
    "        if (episode + 1) % log_interval == 0:\n",
    "            # Action statistics\n",
    "            monitoring['action_counts_per_interval'].append(action_counts.copy())\n",
    "            \n",
    "            # Q-value statistics\n",
    "            monitoring['q_stats']['min'].append(np.min(Q))\n",
    "            monitoring['q_stats']['max'].append(np.max(Q))\n",
    "            monitoring['q_stats']['mean'].append(np.mean(Q))\n",
    "            monitoring['q_stats']['nonzero_count'].append(np.count_nonzero(Q))\n",
    "            \n",
    "            # Rolling reward mean\n",
    "            recent_rewards = monitoring['episode_rewards'][-100:]\n",
    "            mean_reward = np.mean(recent_rewards)\n",
    "            \n",
    "            print(f\"Episode {episode+1:4d}/{n_episodes} | \"\n",
    "                  f\"Mean Reward (last 100): {mean_reward:7.2f} | \"\n",
    "                  f\"Epsilon: {epsilon:.4f} | \"\n",
    "                  f\"Actions: {action_counts} | \"\n",
    "                  f\"Q-range: [{np.min(Q):.2f}, {np.max(Q):.2f}]\")\n",
    "            \n",
    "            # Reset action counts for next interval\n",
    "            action_counts = np.zeros(N_ACTIONS, dtype=int)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return Q, monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f071fc2",
   "metadata": {},
   "source": [
    "## Step 3: Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a9f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "Q_table, monitoring = train_q_learning_monitored(\n",
    "    env=env,\n",
    "    n_episodes=TRAIN_CONFIG['n_episodes'],\n",
    "    alpha=TRAIN_CONFIG['alpha'],\n",
    "    gamma=TRAIN_CONFIG['gamma'],\n",
    "    epsilon_start=TRAIN_CONFIG['epsilon_start'],\n",
    "    epsilon_end=TRAIN_CONFIG['epsilon_end'],\n",
    "    epsilon_decay=TRAIN_CONFIG['epsilon_decay'],\n",
    "    n_soil_bins=TRAIN_CONFIG['n_soil_bins'],\n",
    "    log_interval=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe0db64",
   "metadata": {},
   "source": [
    "## Step 4: Extract Q-Table and Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08184adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Q-table structure\n",
    "print(\"Q-TABLE SHAPE:\", Q_table.shape)\n",
    "print(f\"Total Q-values: {Q_table.size}\")\n",
    "print(f\"Non-zero Q-values: {np.count_nonzero(Q_table)}\")\n",
    "print(f\"Q-value range: [{np.min(Q_table):.3f}, {np.max(Q_table):.3f}]\")\n",
    "print(f\"Q-value mean: {np.mean(Q_table):.3f}\")\n",
    "print(f\"Q-value std: {np.std(Q_table):.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE Q-VALUES (first 10 states)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'State':<7} {'Action 0':<12} {'Action 1':<12} {'Action 2':<12} {'Best Action'}\")\n",
    "print(\"-\"*70)\n",
    "for state in range(min(10, Q_table.shape[0])):\n",
    "    soil_bin, crop_stage, et0_bin, rain_bin = from_discrate_to_full_state(state, TRAIN_CONFIG['n_soil_bins'])\n",
    "    best_action = np.argmax(Q_table[state, :])\n",
    "    print(f\"{state:<7} {Q_table[state, 0]:11.3f}  {Q_table[state, 1]:11.3f}  {Q_table[state, 2]:11.3f}  {best_action}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract deterministic policy using new API\n",
    "policy = extract_policy(Q_table)\n",
    "\n",
    "# Display policy using new API function\n",
    "print_policy(policy, n_soil_bins=TRAIN_CONFIG['n_soil_bins'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c989edb",
   "metadata": {},
   "source": [
    "## Step 5: Policy Analysis & Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a64580",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"POLICY SUMMARY & INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Action frequency in policy\n",
    "action_freq = np.bincount(policy, minlength=N_ACTIONS)\n",
    "print(f\"\\n1. ACTION FREQUENCY IN LEARNED POLICY:\")\n",
    "print(f\"   Action 0 (No irrigation): {action_freq[0]:2d} states ({100*action_freq[0]/len(policy):.1f}%)\")\n",
    "print(f\"   Action 1 (5mm):          {action_freq[1]:2d} states ({100*action_freq[1]/len(policy):.1f}%)\")\n",
    "print(f\"   Action 2 (15mm):         {action_freq[2]:2d} states ({100*action_freq[2]/len(policy):.1f}%)\")\n",
    "\n",
    "# Analysis by soil bin\n",
    "print(f\"\\n2. POLICY BY SOIL BIN:\")\n",
    "for soil_bin in range(3):\n",
    "    states_in_bin = [s for s in range(len(policy)) \n",
    "                     if from_discrate_to_full_state(s, TRAIN_CONFIG['n_soil_bins'])[0] == soil_bin]\n",
    "    actions_in_bin = [policy[s] for s in states_in_bin]\n",
    "    action_counts = np.bincount(actions_in_bin, minlength=N_ACTIONS)\n",
    "    \n",
    "    sm_range = f\"[{soil_bin/3:.3f}, {(soil_bin+1)/3:.3f})\"\n",
    "    print(f\"   Soil bin {soil_bin} (SM {sm_range}): \", end=\"\")\n",
    "    print(f\"No-irr={action_counts[0]}, 5mm={action_counts[1]}, 15mm={action_counts[2]}\")\n",
    "\n",
    "# Analysis by crop stage\n",
    "print(f\"\\n3. POLICY BY CROP STAGE:\")\n",
    "for crop_stage in range(3):\n",
    "    states_in_stage = [s for s in range(len(policy)) \n",
    "                       if from_discrate_to_full_state(s, TRAIN_CONFIG['n_soil_bins'])[1] == crop_stage]\n",
    "    actions_in_stage = [policy[s] for s in states_in_stage]\n",
    "    action_counts = np.bincount(actions_in_stage, minlength=N_ACTIONS)\n",
    "    \n",
    "    stage_name = ['Early', 'Mid', 'Late'][crop_stage]\n",
    "    print(f\"   Stage {crop_stage} ({stage_name}): \", end=\"\")\n",
    "    print(f\"No-irr={action_counts[0]}, 5mm={action_counts[1]}, 15mm={action_counts[2]}\")\n",
    "\n",
    "# State visitation analysis\n",
    "print(f\"\\n4. STATE VISITATION DURING TRAINING:\")\n",
    "visited_states = np.sum(monitoring['state_visit_counts'] > 0)\n",
    "never_visited = np.sum(monitoring['state_visit_counts'] == 0)\n",
    "print(f\"   States visited: {visited_states}/{len(monitoring['state_visit_counts'])}\")\n",
    "print(f\"   States never visited: {never_visited}\")\n",
    "\n",
    "if never_visited > 0:\n",
    "    unvisited_indices = np.where(monitoring['state_visit_counts'] == 0)[0]\n",
    "    print(f\"   Unvisited states: {list(unvisited_indices)}\")\n",
    "\n",
    "# Visit statistics by soil bin\n",
    "print(f\"\\n5. VISIT DISTRIBUTION BY SOIL BIN:\")\n",
    "for soil_bin in range(3):\n",
    "    states_in_bin = [s for s in range(len(policy)) \n",
    "                     if from_discrate_to_full_state(s, TRAIN_CONFIG['n_soil_bins'])[0] == soil_bin]\n",
    "    visits_in_bin = np.sum([monitoring['state_visit_counts'][s] for s in states_in_bin])\n",
    "    total_visits = np.sum(monitoring['state_visit_counts'])\n",
    "    pct = 100 * visits_in_bin / total_visits if total_visits > 0 else 0\n",
    "    print(f\"   Soil bin {soil_bin}: {visits_in_bin:7d} visits ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEHAVIORAL INTERPRETATION:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Interpret learned behavior\n",
    "low_sm_irrigate = sum([1 for s in range(len(policy)) \n",
    "                       if from_discrate_to_full_state(s, TRAIN_CONFIG['n_soil_bins'])[0] == 0 \n",
    "                       and policy[s] > 0])\n",
    "high_sm_no_irrigate = sum([1 for s in range(len(policy)) \n",
    "                           if from_discrate_to_full_state(s, TRAIN_CONFIG['n_soil_bins'])[0] == 2 \n",
    "                           and policy[s] == 0])\n",
    "\n",
    "print(f\"✓ Low soil moisture (bin 0) irrigation: {low_sm_irrigate}/12 states use irrigation\")\n",
    "print(f\"✓ High soil moisture (bin 2) conservation: {high_sm_no_irrigate}/12 states avoid irrigation\")\n",
    "\n",
    "if action_freq[1] > 0 or action_freq[2] > 0:\n",
    "    print(f\"✓ Agent learned to use irrigation actions (not defaulting to no-irrigation)\")\n",
    "else:\n",
    "    print(f\"⚠ Agent only uses no-irrigation (possible reward shaping issue)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a606fdbf",
   "metadata": {},
   "source": [
    "## Step 6: Training Progression Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Reward progression\n",
    "ax = axes[0, 0]\n",
    "episodes = np.arange(len(monitoring['episode_rewards']))\n",
    "ax.plot(episodes, monitoring['episode_rewards'], alpha=0.3, label='Episode reward')\n",
    "# Rolling mean\n",
    "window = 100\n",
    "rolling_mean = np.convolve(monitoring['episode_rewards'], \n",
    "                           np.ones(window)/window, mode='valid')\n",
    "ax.plot(episodes[window-1:], rolling_mean, linewidth=2, label=f'Rolling mean ({window})')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Total Reward')\n",
    "ax.set_title('Reward Progression')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Action usage over time\n",
    "ax = axes[0, 1]\n",
    "intervals = np.arange(len(monitoring['action_counts_per_interval'])) * 500\n",
    "action_data = np.array(monitoring['action_counts_per_interval'])\n",
    "ax.plot(intervals, action_data[:, 0], marker='o', label='Action 0 (No irr)')\n",
    "ax.plot(intervals, action_data[:, 1], marker='s', label='Action 1 (5mm)')\n",
    "ax.plot(intervals, action_data[:, 2], marker='^', label='Action 2 (15mm)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Action Count (per 500 episodes)')\n",
    "ax.set_title('Action Usage Over Time')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Q-value evolution\n",
    "ax = axes[1, 0]\n",
    "intervals = np.arange(len(monitoring['q_stats']['min'])) * 500\n",
    "ax.plot(intervals, monitoring['q_stats']['min'], label='Min Q-value')\n",
    "ax.plot(intervals, monitoring['q_stats']['mean'], label='Mean Q-value')\n",
    "ax.plot(intervals, monitoring['q_stats']['max'], label='Max Q-value')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Q-value')\n",
    "ax.set_title('Q-Value Evolution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Exploration rate (epsilon)\n",
    "ax = axes[1, 1]\n",
    "episodes = np.arange(len(monitoring['epsilon_history']))\n",
    "ax.plot(episodes, monitoring['epsilon_history'], linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Epsilon')\n",
    "ax.set_title('Exploration Rate Decay')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training progression plots generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b21046",
   "metadata": {},
   "source": [
    "---\n",
    "# PHYSICAL REGIME SHIFT EXPERIMENT: Moderate Rain Regime\n",
    "\n",
    "**Objective:** Compare learned policies under different climatic regimes by retraining the agent with increased rainfall.\n",
    "\n",
    "**Hypothesis:** Higher rainfall will reduce irrigation frequency and change policy behavior, particularly in rain-present states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cec177",
   "metadata": {},
   "source": [
    "## [Remaining regime comparison cells would continue here...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64b032",
   "metadata": {},
   "source": [
    "## Archived Note\n",
    "\n",
    "This archive contains the full experimental history, including:\n",
    "- Physical stability calibration methodology\n",
    "- Monitored training loop implementation\n",
    "- Detailed Q-value evolution tracking\n",
    "- Rain regime comparison experiments\n",
    "- State coverage experiments\n",
    "\n",
    "For the clean, final experimental workflow, see the main `experiments.ipynb` notebook."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
